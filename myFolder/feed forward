import numpy as np

text = ["Sara", "Mohamed", "Ibrahim", "Fahem"]
word_to_ix = {word: i for i, word in enumerate(text)}
ix_to_word = {i: word for word, i in word_to_ix.items()}

vocab_size = len(word_to_ix)
embedding_dim = vocab_size  # using one-hot vectors
hidden_size = 8

def one_hot(word_idx, vocab_size):
    vec = np.zeros((vocab_size, 1))
    vec[word_idx] = 1
    return vec

inputs = [one_hot(word_to_ix[word], vocab_size) for word in text[:3]]
target = one_hot(word_to_ix[text[3]], vocab_size)

Wxh = np.random.randn(hidden_size, vocab_size) * 0.01
Whh = np.random.randn(hidden_size, hidden_size) * 0.01
Why = np.random.randn(vocab_size, hidden_size) * 0.01
bh = np.zeros((hidden_size, 1))
by = np.zeros((vocab_size, 1))

h_prev = np.zeros((hidden_size, 1))
hs = []

for x in inputs:
    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h_prev) + bh)
    hs.append(h)
    h_prev = h

y = np.dot(Why, h) + by
y_pred = np.exp(y) / np.sum(np.exp(y))  # softmax

print("Prediction (Probabilities):", y_pred.ravel())
print("Predicted word:", ix_to_word[np.argmax(y_pred)])
